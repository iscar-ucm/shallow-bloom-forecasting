{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdad161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import copy\n",
    "import IPython\n",
    "\n",
    "datos_boya = pd.read_excel('./path_to_data.xlsx') #Read first sheet data\n",
    "\n",
    "\n",
    "#TRAIN FLAG 0=load 1=train new model\n",
    "TRAIN = 0\n",
    "\n",
    "# Drop no valid data\n",
    "datos_boya_no_zero = datos_boya.replace(0,np.nan)\n",
    "datos_boya_proc = datos_boya_no_zero.dropna(subset=[\"T\",\"DO\",\"PH\",\"FC_IVF\"], how=\"all\").reset_index(drop=True)\n",
    "datos_boya_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arrays for the NN input\n",
    "in0_d = (pd.DatetimeIndex(datos_boya_proc[\"date\"]).month).values.reshape((len(datos_boya_proc[\"date\"]),1))\n",
    "in2_T = datos_boya_proc[\"T\"].values.reshape((len(datos_boya_proc[\"T\"]),1))\n",
    "in5_DO = datos_boya_proc[\"DO\"].values.reshape((len(datos_boya_proc[\"DO\"]),1))\n",
    "in6_PH = datos_boya_proc[\"PH\"].values.reshape((len(datos_boya_proc[\"PH\"]),1))\n",
    "in10_FC = datos_boya_proc[\"FC_IVF\"].values.reshape((len(datos_boya_proc[\"FC_IVF\"]),1))\n",
    "\n",
    "# Only selected parameters\n",
    "real_dataset = np.hstack((in0_d, in2_T, in5_DO, in6_PH, in10_FC))\n",
    "\n",
    "\n",
    "print(\"The shape of the dataset is\")\n",
    "np.shape(real_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee04b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full time series for the 5 years\n",
    "# First extract total time\n",
    "start_time = datos_boya_proc[\"date\"][0]\n",
    "end_time = datos_boya_proc[\"date\"][np.shape(real_dataset)[0]-1]\n",
    "\n",
    "# Compute number of 5 min time steps within these two dates\n",
    "total_time_steps = int((end_time-start_time).total_seconds() / 60 / 5)\n",
    "\n",
    "# Create empty full time series array\n",
    "full_dataset = np.empty((int(total_time_steps), np.shape(real_dataset)[1]))\n",
    "full_dataset[:]=np.nan\n",
    "\n",
    "np.shape(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill full_dataset with real values. Create a list with each point being 5 min and add the real values to the corresponding position.\n",
    "full_dataset[0]=real_dataset[0]; full_data_index=0; i=1; loosed_minutes=0\n",
    "while(i<np.shape(real_dataset)[0]):\n",
    "    # Get num time slots (5min) between the two values\n",
    "    time_diff = (datos_boya_proc[\"date\"][i] - datos_boya_proc[\"date\"][i-1]).total_seconds()\n",
    "    #print(loosed_minutes)\n",
    "    time_jump = (time_diff+loosed_minutes*60)/60/5\n",
    "    loosed_minutes = (time_diff+loosed_minutes*60)/60%5\n",
    "    #print(i,time_jump, loosed_minutes, time_diff,(time_diff+loosed_minutes*60))\n",
    "    \n",
    "    # If number of slots is 0 it means data with a bad label, deal with it\n",
    "    if(time_jump==0):\n",
    "        num_same_values = 0\n",
    "        # Advance until no data with same label\n",
    "        while(((datos_boya_proc[\"date\"][i] - datos_boya_proc[\"date\"][i-1]).total_seconds()/60/5)==0):\n",
    "            i+=1\n",
    "            num_same_values +=1\n",
    "            \n",
    "        time_diff = (datos_boya_proc[\"date\"][i] - datos_boya_proc[\"date\"][i-1]).total_seconds()\n",
    "        time_jump=(time_diff+loosed_minutes*60)/60/5\n",
    "        loosed_minutes = (time_diff+loosed_minutes*60)%5\n",
    "        \n",
    "        # Place the same label values along the free slot equally distributed max 1 hour \n",
    "        same_time_jump = int(min(int(time_jump),12)/num_same_values)\n",
    "        if (same_time_jump!=0): # If we have more points than slots within an hour dont trust the data\n",
    "            for l in range(num_same_values):\n",
    "                full_dataset[full_data_index+l*same_time_jump] = real_dataset[i+(l-num_same_values-1)]\n",
    "        \n",
    "    time_jump_round = int(time_jump)\n",
    "    full_data_index += time_jump_round\n",
    "    full_dataset[full_data_index] = real_dataset[i]\n",
    "    i+=1\n",
    "\n",
    "\n",
    "full_dataset_t = np.transpose(full_dataset)\n",
    "y = full_dataset_t[1]\n",
    "x = range(np.shape(full_dataset_t)[1])\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"Time (5min/step)\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate to fill the full_dataset time series (https://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array)\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "full_dataset_interp = copy.deepcopy(full_dataset)\n",
    "full_dataset_t = np.transpose(full_dataset_interp)\n",
    "for i in range(np.shape(full_dataset_t)[0]):\n",
    "    nans, x= nan_helper(full_dataset_t[i])\n",
    "    if(i == 0): # The 0 feature is the month and \"cant\" be decimal\n",
    "        #print(\"Flooring the month\")\n",
    "        full_dataset_t[i][nans]= np.floor(np.interp(x(nans), x(~nans), full_dataset_t[i][~nans]))\n",
    "    else:\n",
    "        #print(\"Not flooring the rest\")\n",
    "        full_dataset_t[i][nans]= np.interp(x(nans), x(~nans), full_dataset_t[i][~nans])\n",
    "                                  \n",
    "full_dataset_interp = np.transpose(full_dataset_t)\n",
    "\n",
    "full_dataset_t = np.transpose(full_dataset_interp)\n",
    "y= full_dataset_t[1]#[106000:108000]\n",
    "x = range(np.shape(full_dataset_t)[1])#(2000)\n",
    "plt.xlabel(\"Time (5min/step)\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# the scaler object (model)\n",
    "scaler = StandardScaler() # fit and transform the data\n",
    "full_dataset_interp_norm = scaler.fit_transform(full_dataset_interp)\n",
    "\n",
    "print(np.shape(full_dataset_interp_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97039d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to csv\n",
    "#pd.DataFrame(full_dataset_interp_norm).to_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c7b7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets create dataset for training and testing\n",
    "# First create a time column list for the time\n",
    "import datetime\n",
    "time_column = []\n",
    "for i in range(total_time_steps): #total_time_steps = int((end_time-start_time).total_seconds() / 60 / 5)\n",
    "    time_column.append(start_time + datetime.timedelta(minutes=5*i))\n",
    "\n",
    "# Create input/output samples\n",
    "def split_sequences(sequences, sequences_interp, time_seq, in_window, step, pred_window, in_freq, out_freq):\n",
    "    X_train, y_train, time_train, time_train_out = list(), list(), list(), list()\n",
    "    X_test, y_test, time_test, time_test_out = list(), list(), list(), list()\n",
    "    i=0; test_flag = 0; test_date = datetime.datetime(2014, 5, 1) # test date in yyyy/mm/dd format. This day starts testing\n",
    "    while(i<len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + in_window\n",
    "        end_out_ix = end_ix + pred_window\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_out_ix > len(sequences):\n",
    "            break\n",
    "               \n",
    "        # gather input and output parts of the pattern\n",
    "        in_seq, out_seq = sequences_interp[i:end_ix], sequences_interp[end_ix:end_out_ix, -1]\n",
    "        time_seq = time_column[i:end_ix]\n",
    "        time_seq_out = time_column[end_ix:end_out_ix]\n",
    "        \n",
    "        # Lets fill things\n",
    "        X_tmp, y_tmp = list(), list()\n",
    "        # Fill with mean\n",
    "        for k in range(1,len(in_seq)+1):\n",
    "            if (k%in_freq) == 0:\n",
    "                X_tmp.append(np.mean(in_seq[(k-in_freq):k], axis = 0))   \n",
    "        for k in range(1,len(out_seq)+1):   \n",
    "            if (k%out_freq) == 0:\n",
    "                y_tmp.append(np.mean(out_seq[(k-out_freq):k], axis = 0))\n",
    "        if(time_column[i] < test_date): #Train\n",
    "            X_train.append(X_tmp)\n",
    "            y_train.append(y_tmp)\n",
    "            time_train.append(time_seq[in_freq::in_freq])\n",
    "            time_train_out.append(time_seq_out[0:-out_freq:out_freq]) \n",
    "        else: #Test\n",
    "            if (test_flag == 0):\n",
    "                print(\"Changed year at index: \", i)\n",
    "                changed_year_index = i\n",
    "                test_flag = 1\n",
    "            X_test.append(X_tmp)\n",
    "            y_test.append(y_tmp)\n",
    "            time_test.append(time_seq[0::in_freq])\n",
    "            time_test_out.append(time_seq_out[0::out_freq]) \n",
    "        \n",
    "        # Next step\n",
    "        i+=step\n",
    "        \n",
    "    return np.array(X_train), np.array(y_train), time_train, np.array(X_test), np.array(y_test), time_test, time_train_out, time_test_out, changed_year_index\n",
    " \n",
    "step = 12; in_steps = 8064; pred_steps = 8064; in_freq = 288; out_freq=288\n",
    "X_train, y_train, time_train, X_test, y_test, time_test, time_train_out, time_test_out, changed_year_index = split_sequences(full_dataset, full_dataset_interp_norm, time_column, in_steps, step, pred_steps, in_freq, out_freq)\n",
    "\n",
    "in_window = int(in_steps/in_freq); pred_window = int(pred_steps/out_freq)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))\n",
    "print(\"The in window is: \", in_window)\n",
    "print(\"The out window is: \", pred_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd44469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data aug and perform it (https://github.com/uchidalab/time_series_augmentation/)\n",
    "import sys\n",
    "sys.path.insert(1, '/home/raul/repos/time_series_augmentation') #(https://stackoverflow.com/questions/4383571/importing-files-from-different-folder)\n",
    "from utils.input_data import read_data_sets\n",
    "import utils.datasets as ds\n",
    "import utils.augmentation as aug\n",
    "import utils.helper as hlp\n",
    "\n",
    "# Lets data aug\n",
    "X_train_jitter = aug.jitter(X_train)\n",
    "X_train_jitter_2 = aug.jitter(X_train, sigma=0.1)\n",
    "X_train_jitter_3 = aug.jitter(X_train, sigma=0.2)\n",
    "X_train_scale = aug.scaling(X_train, sigma=0.6)\n",
    "X_train_scale_2 = aug.scaling(X_train, sigma=1)\n",
    "X_train_magwarp = aug.magnitude_warp(X_train, sigma=1, knot=4)\n",
    "X_train_magwarp_2 = aug.magnitude_warp(X_train, sigma=0.6, knot=14)\n",
    "X_train_timewarp = aug.time_warp(X_train, sigma=0.4, knot=4)\n",
    "X_train_timewarp_2 = aug.time_warp(X_train, sigma=0.2, knot=14)\n",
    "\n",
    "# Add to train dataset\n",
    "X_train_aug = np.concatenate((X_train, X_train_jitter, X_train_jitter_2, X_train_jitter_3, X_train_scale, X_train_scale_2, X_train_magwarp, X_train_magwarp_2, X_train_timewarp, X_train_timewarp_2))\n",
    "y_train_aug = np.concatenate((y_train, y_train, y_train, y_train, y_train , y_train, y_train, y_train, y_train, y_train))\n",
    "\n",
    "print(np.shape(X_train_aug))\n",
    "print(np.shape(y_train_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data aug example\n",
    "hlp.plot1d(X_train[0], X_train_jitter[0])\n",
    "hlp.plot1d(X_train[0], X_train_jitter_2[0])\n",
    "hlp.plot1d(X_train[0], X_train_jitter_3[0])\n",
    "hlp.plot1d(X_train[0], X_train_scale[0])\n",
    "hlp.plot1d(X_train[0], X_train_scale_2[0])\n",
    "hlp.plot1d(X_train[0], X_train_magwarp[0])\n",
    "hlp.plot1d(X_train[0], X_train_magwarp_2[0])\n",
    "hlp.plot1d(X_train[0], X_train_timewarp[0])\n",
    "hlp.plot1d(X_train[0], X_train_timewarp_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras\n",
    "\n",
    "# First define num features\n",
    "n_features = X_train_aug.shape[2]\n",
    "\n",
    "if TRAIN==1:\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(in_window, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(pred_window))\n",
    "    optimizer = keras.optimizers.Adam(lr=0.000005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    batch_size = 100\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30) # early stop for validation\n",
    "    history = model.fit(X_train_aug, y_train_aug, batch_size=batch_size , epochs=1000, validation_data=(X_test,y_test), callbacks=[es])\n",
    "\n",
    "    # Save the model\n",
    "    model.save('modelo.h5')\n",
    "else:\n",
    "    model = load_model('../models/model.h5')\n",
    "    # summarize model.\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot full trajectory every 5 min\n",
    "# Plot sequence\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define some useful functions\n",
    "def revert_out_norm(y_norm):\n",
    "    # This function takes the output of the network and perform the inverse normalization operation\n",
    "    y_raw = list()\n",
    "    for i in range(len(y_norm)):\n",
    "        y_raw_tmp = list()\n",
    "        for j in range(len(y_norm[i])):\n",
    "            y_raw_tmp.append(scaler.inverse_transform(np.pad([y_norm[i,:][j]], (n_features-1,0)).reshape(1, -1))[0,n_features-1])\n",
    "        y_raw.append(y_raw_tmp)\n",
    "    return y_raw\n",
    "\n",
    "def remove_duplicates(sequences, time_info, index):\n",
    "    # This function remove data that is duplicated in time from sequences\n",
    "    # Sequences is a NxM array of arrays. Where N is the num os samples and M is the number of pred days.\n",
    "    # time_info is the time info for each day\n",
    "    # Index is the day index we want to use\n",
    "    pred_steps = index*24; # Number of predicted steps. Before this steps we have no info.\n",
    "    clean_seq = []; clean_time = [];\n",
    "    # Init to first step \n",
    "    clean_seq.append(sequences[0][0])\n",
    "    clean_time.append(time_info[0][0])\n",
    "    for i in range(pred_steps): #The first index days steps are empty\n",
    "        clean_seq.append(sequences[0][0])\n",
    "        clean_time.append(time_info[i][0])\n",
    "\n",
    "    # Continue with the rest\n",
    "    for i in range(1, (len(sequences)-pred_steps-1)):\n",
    "        if(time_info[i][index]-clean_time[-1]).total_seconds()>0:\n",
    "            clean_seq.append(sequences[i][index])\n",
    "            clean_time.append(time_info[i][index])\n",
    "    return clean_seq, clean_time\n",
    "\n",
    "def interpolate(inp, fi):\n",
    "    i, f = int(fi // 1), fi % 1  # Split floating-point index into whole & fractional parts.\n",
    "    j = i+1 if f > 0 else i  # Avoid index error.\n",
    "    return (1-f) * inp[i] + f * inp[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653b632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Revert normalization\n",
    "y_train_raw = revert_out_norm(y_train)\n",
    "y_test_raw = revert_out_norm(y_test)\n",
    "y_train_pred_raw = revert_out_norm(y_train_pred)\n",
    "y_test_pred_raw = revert_out_norm(y_test_pred)\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Clean duplicates. The architecture give us a 28 day sequence for each 28 day sequence we have. So we have a lot of days duplicated.\n",
    "y_train_clean, clean_train_time = remove_duplicates(y_train_raw, time_train_out, index)\n",
    "y_train_pred_clean, clean_train_pred_time = remove_duplicates(y_train_pred_raw, time_train_out, index)\n",
    "\n",
    "y_test_clean, clean_test_time = remove_duplicates(y_test_raw, time_test_out, index)\n",
    "y_test_pred_clean, clean_test_pred_time = remove_duplicates(y_test_pred_raw, time_test_out, index)\n",
    "\n",
    "print(np.shape(y_test_clean))\n",
    "print(np.shape(y_test_pred_clean))\n",
    "\n",
    "########## Plot pred_train vs train #######\n",
    "if(len(y_train_pred_clean)!=len(y_train_clean)):\n",
    "    print(\"Warning plotting two traj with different lengths in same place\")\n",
    "\n",
    "plt.plot(y_train_clean, label=\"true\")   \n",
    "plt.plot(y_train_pred_clean, label=\"estimated\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step (60min/step)\")\n",
    "plt.ylabel(\"Ficocianinas (ug/L)\")\n",
    "plt.savefig(\"pred-train.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "########## Plot pred_train vs full train trajectory #########\n",
    "full_base_traj = full_dataset_interp[in_steps:changed_year_index,-1]\n",
    "\n",
    "# Pred\n",
    "delta = (len(y_train_pred_clean)-1) / (len(full_base_traj)-1)\n",
    "outp = [interpolate(y_train_pred_clean, i*delta) for i in range(len(full_base_traj))]\n",
    "pad_zeros = np.zeros(in_steps+int(pred_steps/2)) # The first in_steps are not predicted, then the next step are moved +-1/2 of the average due to being the average of pred_window\n",
    "outp_pred = np.concatenate((pad_zeros, outp))\n",
    "\n",
    "if(len(outp_pred)!=len(full_dataset_interp[:changed_year_index,-1])):\n",
    "    print(\"Warning plotting two traj with different lengths in same place\")\n",
    "    \n",
    "plt.ylim([0, 40])\n",
    "plt.plot(full_dataset_interp[:changed_year_index,-1], label=\"true\")\n",
    "plt.plot(outp_pred, label=\"estimated\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step (60min/step)\")\n",
    "plt.ylabel(\"Ficocianinas (ug/L)\")\n",
    "plt.savefig(\"pred-train-full.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "########## Plot pred_test vs test #######\n",
    "\n",
    "if(len(y_test_pred_clean)!=len(y_test_clean)):\n",
    "    print(\"Warning plotting two traj with different lengths in same place\")\n",
    "\n",
    "plt.plot(y_test_clean, label=\"true\")\n",
    "plt.plot(y_test_pred_clean, label=\"estimated\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step (60min/step)\")\n",
    "plt.ylabel(\"Ficocianinas (ug/L)\")\n",
    "plt.savefig(\"pred-test.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "########## Plot pred_test vs full test trajectory #########\n",
    "full_base_traj = full_dataset_interp[in_steps+changed_year_index:,-1]\n",
    "\n",
    "# Pred\n",
    "delta = (len(y_test_pred_clean)-1) / (len(full_base_traj)-1)\n",
    "outp = [interpolate(y_test_pred_clean, i*delta) for i in range(len(full_base_traj))]\n",
    "\n",
    "if(len(outp)!=len(full_dataset_interp[changed_year_index:,-1])):\n",
    "    print(\"Warning plotting two traj with different lengths in same place\")\n",
    "    \n",
    "plt.ylim([0, 40])\n",
    "plt.plot(full_dataset_interp[changed_year_index:,-1], label=\"true\")\n",
    "plt.plot(outp, label=\"estimated\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step (60min/step)\")\n",
    "plt.ylabel(\"Ficocianinas (ug/L)\")\n",
    "plt.savefig(\"pred-test-full.pdf\")\n",
    "plt.show()\n",
    "\n",
    "print(np.shape(y_test_pred_clean))\n",
    "print(np.shape(y_test_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ce98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_clean_memory = list()\n",
    "clean_test_pred_time_memory = list()\n",
    "y_test_clean, clean_train_pred_time = remove_duplicates(y_test_raw, time_test_out, 0)\n",
    "for i in range(np.shape(y_test_raw)[1]):\n",
    "    # The result is aligned because each result has an absolute time value\n",
    "    y_test_pred_clean, clean_test_pred_time = remove_duplicates(y_test_pred_raw, time_test_out, i) \n",
    "    y_test__clean, clean_test_pred_time = remove_duplicates(y_test_raw, time_test_out, i)\n",
    "    \n",
    "    if(len(y_test_pred_clean)!=len(y_test_clean)):\n",
    "        print(\"Warning plotting two traj with different lengths in same place\")\n",
    "    \n",
    "    y_test_pred_clean_memory.append(y_test_pred_clean)\n",
    "    clean_test_pred_time_memory.append(clean_test_pred_time)\n",
    "    \n",
    "    plt.plot(y_test_clean, label=\"true\")\n",
    "    plt.plot(y_test_pred_clean, label=\"estimated\")\n",
    "    plt.xlabel(\"Time step (60min/step)\")\n",
    "    plt.ylabel(\"Ficocianinas (ug/L)\")\n",
    "    plt.savefig(\"pred_\"+str(i)+\".pdf\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b50d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(y_test_pred_clean))\n",
    "print(np.shape(y_test_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage error \n",
    "y_test_clean = np.array(y_test_clean)\n",
    "percentage_hist = []\n",
    "\n",
    "for i in range(len(y_test_pred_clean_memory)):\n",
    "    y_test_pred_clean_memory[i] = np.array(y_test_pred_clean_memory[i])\n",
    "    # Compute the percentage\n",
    "    percentage = 100-abs(y_test_clean-y_test_pred_clean_memory[i])/y_test_clean*100\n",
    "    percentage = sum(percentage)/len(percentage)\n",
    "    percentage_hist.append(percentage)\n",
    "\n",
    "plt.plot(percentage_hist)\n",
    "plt.ylim([50, 100])\n",
    "plt.xlabel(\"Prediction Days\")\n",
    "plt.ylabel(\"Precision %\")\n",
    "plt.savefig(\"precision.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute discrete error\n",
    "threshold_bin = (max(y_test_clean)-min(y_test_clean))/5\n",
    "start = min(y_test_clean)\n",
    "y_test_bin = np.digitize(y_test_clean,[start, start+threshold_bin, start+threshold_bin*2, start+threshold_bin*3, start+threshold_bin*4])\n",
    "percentage_hist_bin = []\n",
    "\n",
    "for i in range(len(y_test_pred_clean_memory)):\n",
    "    y_test_pred_bin = np.digitize(y_test_pred_clean_memory[i],[start, start+threshold_bin, start+threshold_bin*2, start+threshold_bin*3, start+threshold_bin*4])\n",
    "    # Compute the percentage\n",
    "    percentage = 100-abs(y_test_bin-y_test_pred_bin)/y_test_bin*100\n",
    "    percentage = sum(percentage)/len(percentage)\n",
    "    percentage_hist_bin.append(percentage)\n",
    "\n",
    "plt.plot(percentage_hist_bin)\n",
    "plt.xlabel(\"Prediction Days\")\n",
    "plt.ylabel(\"Precision %\")\n",
    "plt.savefig(\"precision-bin.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#permutation importance session#\n",
    "################################\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import IPython\n",
    "\n",
    "randsample = 10\n",
    "\n",
    "errreal = mean_squared_error(y_test_raw, y_test_pred_raw)\n",
    "\n",
    "# For each sample shape (1,in_window,n_features) one feature is replaced for random values \n",
    "permutsample = np.zeros((randsample, in_window, n_features))\n",
    "\n",
    "for trying in range(randsample): \n",
    "    randval = np.zeros((len(y_test), in_window)) # For each output we create a random input with input size\n",
    "    print(\" \")\n",
    "    print(\"Iteracion: \", trying, end = '')\n",
    "    for i in range(len(y_test)):\n",
    "        randval[i] = np.random.uniform(-1,1, in_window)\n",
    "    \n",
    "    for i in range(np.shape(X_test)[2]):\n",
    "        print(\".\", end = '')\n",
    "        permutinput = np.zeros(np.shape(X_test))\n",
    "        permutinput[:] = X_test\n",
    "        permutinput[:,:,i] = randval\n",
    "        y_pred_rand = model.predict(permutinput)\n",
    "        y_pred_rand_raw = revert_out_norm(y_pred_rand)\n",
    "        err = mean_squared_error(y_test_raw, y_pred_rand_raw)\n",
    "        permutsample[trying, i] = err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fffcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print mean and standard deviation of error\n",
    "errperformance = np.zeros((np.shape(X_test)[2], 2))\n",
    "for i in range(np.shape(X_test)[2]):\n",
    "    errperformance[i,0] = np.mean(permutsample[:,i])\n",
    "    errperformance[i,1] = np.std(permutsample[:,i])\n",
    "errperformance[:,0] = errreal - errperformance[:,0]\n",
    "\n",
    "# Full\n",
    "param_dict = [\"date\", \"D\", \"T\", \"C\", \"C25\", \"DO\", \"PH\", \"ORP\", \"CHLA_IVF\", \"CDOMt\", \"FC_IVF\"]\n",
    "\n",
    "# As the boat\n",
    "param_dict = [\"date\", \"T\", \"DO\", \"PH\", \"FC_IVF\"] \n",
    "\n",
    "for i in range(len(param_dict)):\n",
    "    print(param_dict[i], errperformance[i]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = range(len(errperformance))\n",
    "param_dict_plot = ('date', 'D','T','C','C25','DO','PH','ORP','CHLA','CDOM', 'FC')\n",
    "param_dict_plot = ('date', 'T', 'DO', 'PH', 'FC_IVF')\n",
    "plt.bar(y_pos, -errperformance[:,0])\n",
    "plt.xticks(y_pos, param_dict_plot)\n",
    "plt.savefig(\"input-weight.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "errperformance[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save things in text \n",
    "with open('data.txt', 'w') as f:\n",
    "    f.write(\"Error/desviacion introducido por cada feature \")\n",
    "    f.write(\"\\n\")\n",
    "    for i in range(len(errperformance)):\n",
    "        f.write(param_dict_plot[i])\n",
    "        f.write(\" \")\n",
    "        f.write(str(errperformance[i][0]*1000))\n",
    "        f.write(\" \")\n",
    "        f.write(str(errperformance[i][1]*1000))\n",
    "        f.write(\"\\n\")\n",
    "    f.write(\"Precision Continua en funcion del dia\\n\")\n",
    "    for i in range(len(percentage_hist)):\n",
    "        f.write(str(percentage_hist[i]))\n",
    "        f.write(\" \")\n",
    "    f.write(\"Precision Discreta en funcion del dia\\n\")\n",
    "    for i in range(len(percentage_hist_bin)):\n",
    "        f.write(str(percentage_hist_bin[i]))\n",
    "        f.write(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == 1:\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"accuracy.pdf\")\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(\"loss.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69449716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
